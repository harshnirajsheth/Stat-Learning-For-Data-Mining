{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>IEE 520: Fall 2019</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Nearest Neighbors (10/03/19)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Klim Drobnyh (klim.drobnyh@asu.edu)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: TO SUPPORT INTERACTIVE PLOTS IN JUPYTER LAB, RUN**\n",
    "\n",
    "conda install -c conda-forge nodejs\n",
    "\n",
    "jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For compatibility with Python 2\n",
    "from __future__ import print_function\n",
    "\n",
    "# To load datasets\n",
    "from sklearn import datasets\n",
    "\n",
    "# To import the classifier (K-Nearest Neighbors Classifier and Regressor)\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "# To measure accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# To support plots\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# To display all the plots inline\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To increase quality of figures\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Classification</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Load the data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems\" as an example of linear discriminant analysis.\n",
    "\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_iris(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Nearest Neighbors Classifier</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a full list of parameters here:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=520)\n",
    "\n",
    "yhat = np.zeros((X.shape[0], ))\n",
    "# Cross-validation\n",
    "for train, test in kfold.split(X, y):\n",
    "    model = KNeighborsClassifier(7, weights='distance')\n",
    "    model.fit(X[train], y[train])\n",
    "    yhat[test] = model.predict(X[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to install pandas_ml in order to use that!\n",
    "# conda install -c conda-forge pandas_ml\n",
    "\n",
    "# Uncomment the next line to install a missing package to Google Colab Environment\n",
    "# !pip install pandas_ml\n",
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrix(y, yhat)\n",
    "ax = cm.plot(backend='seaborn', annot=True, fmt='g')\n",
    "ax.set_title('CV Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Visualization</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're going to use just 2 variables (for visualization purpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use closure to store the related variables\n",
    "def create_plot_knn_classification(_X, _y):\n",
    "    X, y = _X, _y\n",
    "    def plot_knn(k=3, weighted=True):\n",
    "        h = .02  # step size in the mesh\n",
    "        cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "        cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "        if weighted:\n",
    "            clf = KNeighborsClassifier(k, weights='distance')\n",
    "        else:\n",
    "            clf = KNeighborsClassifier(k, weights='uniform')\n",
    "        clf.fit(X, y)\n",
    "        x1_min = X[:, 0].min() - 1\n",
    "        x1_max = X[:, 0].max() + 1\n",
    "        x2_min = X[:, 1].min() - 1\n",
    "        x2_max = X[:, 1].max() + 1\n",
    "        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h),\n",
    "                               np.arange(x2_min, x2_max, h))\n",
    "        Z = clf.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx1.shape)\n",
    "        plt.figure()\n",
    "        plt.pcolormesh(xx1, xx2, Z, cmap=cmap_light)\n",
    "\n",
    "        # Plot also the training points\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                    edgecolor='k', s=20)\n",
    "        plt.xlim(xx1.min(), xx1.max())\n",
    "        plt.ylim(xx2.min(), xx2.max())\n",
    "        plt.title(\"Nearest neighbor classification (k = %i, %s)\"\n",
    "                  % (k, 'weighted' if weighted else 'unweighted'))\n",
    "        plt.show()\n",
    "    return plot_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(create_plot_knn_classification(X, y), k=(1, 150, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Regression (visualization)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a full list of parameters here:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will emulate $y=x^2+\\epsilon$, where $\\epsilon$ is standard normal, $1\\leq x \\leq 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "np.random.seed(520)\n",
    "X = np.random.uniform(1, 10, n)\n",
    "e = np.random.randn(n)\n",
    "y = X**2 + e\n",
    "X = X.reshape((n, 1))\n",
    "y = y.reshape((n,))\n",
    "plt.plot(X, y, 'bo')\n",
    "plt.plot(np.arange(1, 10, 0.1), np.arange(1, 10, 0.1)**2, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use closure to store the related variables\n",
    "def create_plot_knn_regression(_X, _y):\n",
    "    X, y = _X, _y\n",
    "    def plot_knn(k=3, weighted=True):\n",
    "        h = .02  # step size in the mesh\n",
    "        if weighted:\n",
    "            clf = KNeighborsRegressor(k, weights='distance')\n",
    "        else:\n",
    "            clf = KNeighborsRegressor(k, weights='uniform')\n",
    "        clf.fit(X, y)\n",
    "        x_min = X[:, 0].min() - 1\n",
    "        x_max = X[:, 0].max() + 1\n",
    "        xx = np.arange(x_min, x_max, h)\n",
    "        xx = xx.reshape((xx.shape[0], 1))\n",
    "        Z = clf.predict(xx)\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        plt.figure()\n",
    "        plt.plot(xx, Z, 'g')\n",
    "        plt.plot(xx, xx**2, 'r')\n",
    "\n",
    "        # Plot also the training points\n",
    "        plt.plot(X, y, 'bo')\n",
    "        plt.title(\"Nearest neighbor regression (k = %i, %s)\"\n",
    "                  % (k, 'weighted' if weighted else 'unweighted'))\n",
    "        plt.show()\n",
    "    return plot_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(create_plot_knn_regression(X, y), k=(1, n, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
